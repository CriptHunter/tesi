{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7qy_0RayxMy"
   },
   "source": [
    "# CARS recommender system\n",
    "Implementation of the deep NN model described in the paper \"Context-Aware Recommendations Based on Deep\n",
    "Learning Frameworks\".\n",
    "https://dl.acm.org/doi/10.1145/3386243\n",
    "\n",
    "Datasets:\n",
    "- Frappe\n",
    "- Yelp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YasL9GIGF3Jb"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1q4UgOkhyxM4",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # to split dataset in two parts\n",
    "from sklearn.model_selection import KFold # to split dataset using  k-fold cross validation\n",
    "from sklearn.metrics import * # evaluation metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Input, Embedding, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # for creating chart\n",
    "import requests # for downloading the dataset\n",
    "from collections import deque # queue data structure\n",
    "from scipy.cluster.hierarchy import * # for hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection.validation import cross_validate\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzbMVL48MAcU"
   },
   "source": [
    "\n",
    "\n",
    "## Some functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CFVxgFQL_4P",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot loss based on history of model.fit, ymin and ymax are the minimum and maximum values of the y axis\n",
    "def plot_loss(history, ymin=0, ymax=1):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.ylim([ymin, ymax])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "# plot chart of true values on predictions\n",
    "def plot_predictions(test_y, pred_y):\n",
    "  a = plt.axes(aspect='equal')\n",
    "  plt.scatter(test_y, pred_y)\n",
    "  plt.xlabel('True Values')\n",
    "  plt.ylabel('Predictions')\n",
    "  lims = [0, 5]\n",
    "  plt.xlim(lims)\n",
    "  plt.ylim(lims)\n",
    "  _ = plt.plot(lims, lims)\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "# k-fold cross validation object\n",
    "kf = KFold(n_splits=2, random_state=42, shuffle=True)\n",
    "\n",
    "def kfold_train(model, epochs, batch_size, verbose, df, x_labels, y_label, \n",
    "                kf, using_context=False, context_labels=None):\n",
    "    '''\n",
    "    Train a model using K-fold CV\n",
    "\n",
    "    Parameters:\n",
    "        model: the model to be trained\n",
    "        epochs: training epochs for each fold\n",
    "        batch_size: batch size for each fold\n",
    "        verbose: show training batch and loss\n",
    "        df: the dataframe on which the model will be trained\n",
    "        x_labels: features labels\n",
    "        y_label: desired output labels\n",
    "        using_context: if the model incorporates context\n",
    "        context_labels: contextual features labels\n",
    "        kf: sklearn kfold object\n",
    "    '''\n",
    "    idx = 0\n",
    "    rmse = np.empty(kf.n_splits)\n",
    "    mae = np.empty(kf.n_splits)\n",
    "\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        print(f'Training on fold {idx}...')\n",
    "        train_x = df.loc[train_index, x_labels]  # get a dataset subset with df.loc[rows, columns]\n",
    "        train_y = df.loc[train_index, y_label]\n",
    "        if using_context: # if the model supports contextual features\n",
    "            train_context = df.loc[train_index, context_labels]\n",
    "            model.fit([train_x.user, train_x.item, train_context], train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        else:\n",
    "            model.fit([train_x.user, train_x.item], train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "        print(f'Testing on fold {idx}...')\n",
    "        test_x = df.loc[test_index, x_labels]\n",
    "        test_y = df.loc[test_index, y_label]\n",
    "        if using_context:\n",
    "            test_context = df.loc[test_index, context_labels]\n",
    "            pred_y = model.predict([test_x.user, test_x.item, test_context]).flatten()\n",
    "        else:\n",
    "            pred_y = model.predict([test_x.user, test_x.item]).flatten()\n",
    "\n",
    "        rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "        mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "        print(f'RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "\n",
    "        idx = idx + 1\n",
    "    \n",
    "    return np.mean(rmse), np.mean(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load frappe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('final datasets/frappe_final.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_labels = list(df.columns[0:2])\n",
    "y_label = df.columns[2]\n",
    "context_labels = list(df.columns[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYutpX2ryxM-",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# count number of unique users and items\n",
    "n_users, n_items = len(df.user.unique()), len(df.item.unique())\n",
    "n_context = len(context_labels)\n",
    "\n",
    "f'Number of users: {n_users}      Number of apps: {n_items}     Number of context features: {n_context}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('final datasets/yelp_final.csv', sep=\",\")\n",
    "df = df.sample(n=100000)\n",
    "df[df.columns[12:34]] = df[df.columns[12:34]].astype('uint8') # convert one hot encoded columns to uint8\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_labels = list(df.columns[0:2])\n",
    "y_label = df.columns[2]\n",
    "context_labels = list(df.columns[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# count number of unique users and items\n",
    "n_users, n_items = len(df.user.unique()), len(df.item.unique())\n",
    "n_context = len(context_labels)\n",
    "\n",
    "# RISCALARE INDICI USER E ITEM ALTRIMENTI NON VA\n",
    "\n",
    "f'Number of users: {n_users}      Number of business: {n_items}     Number of context features: {n_context}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIfSxcyQzRzn"
   },
   "source": [
    "### Latent context extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OclHf_U97Pr"
   },
   "source": [
    "#### With autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO1R_ZVl783U",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_context_AE, test_context_AE = train_test_split(df.loc[:,context_labels], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iHJfOanzNQy",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# size of the encoded representation\n",
    "n_latent_context = 11\n",
    "\n",
    "# input layer\n",
    "input = Input(shape=(n_context,))\n",
    "# the encoded representation of the input\n",
    "encoded = Dense(n_latent_context, activation='sigmoid')(input)\n",
    "# the reconstruction of the input\n",
    "decoded = Dense(n_context, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = keras.Model(input, decoded)\n",
    "\n",
    "# Only encoder model\n",
    "encoder = keras.Model(input, encoded)\n",
    "\n",
    "# Only Decoder model\n",
    "encoded_input = keras.Input(shape=(n_latent_context,))   # takes as input the encoded context\n",
    "decoder_layer = autoencoder.layers[-1]   # Retrieve the last layer of the autoencoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqmXeqgOZ0KZ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train the autoencoder on the context\n",
    "history = autoencoder.fit(train_context_AE, train_context_AE,\n",
    "                          epochs=50,\n",
    "                          verbose=False,\n",
    "                          batch_size=128,\n",
    "                          validation_data=(test_context_AE, test_context_AE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnPVHgFhOvzx",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(history, ymin=0, ymax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDXtf6_aEm_S",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# weight matrix of neurons that connect input layers to hidden layer\n",
    "# get weight returns a list of weights and biases, by taking weight[0] you extract only the weights\n",
    "weight_matrix = autoencoder.layers[1].get_weights()[0]\n",
    "weight_matrix = np.asarray(weight_matrix)\n",
    "\n",
    "def get_latent_context_AE():\n",
    "    latent_context = np.empty(shape=(df.shape[0], n_latent_context))\n",
    "    latent_context_labels = [f\"latent_{x}\" for x in range(n_latent_context)]\n",
    "    \n",
    "    # multiply each context sample for the weight matrix\n",
    "    for idx, s in enumerate(df.loc[:, context_labels].values):\n",
    "        latent_context[idx] = s @ weight_matrix\n",
    "    \n",
    "    # apply activation function\n",
    "    latent_context = sigmoid(latent_context)\n",
    "    df_latent_context = pd.DataFrame(latent_context, columns=latent_context_labels)\n",
    "\n",
    "    return df_latent_context, latent_context_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV89AtG_-Ms5"
   },
   "source": [
    "#### With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQiYQ_kJ-UoB",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_latent_context_PCA(n_latent_context):\n",
    "    latent_context_labels = [f\"latent_{x}\" for x in range(n_latent_context)]\n",
    "    pca = PCA(n_components=n_latent_context)\n",
    "    pca.fit(df.loc[:,context_labels])\n",
    "    latent_context = pca.transform(df.loc[:,context_labels])  \n",
    "    df_latent_context = pd.DataFrame(latent_context, columns=latent_context_labels)\n",
    "    return df_latent_context, latent_context_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P04efe0tHSi1"
   },
   "source": [
    "#### Run selected method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGbX4jMLHgzQ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "method = 'AE'\n",
    "if method == 'PCA': # latent context with PCA\n",
    "    df_latent_context, latent_context_labels = get_latent_context_PCA(n_latent_context)\n",
    "    df = pd.concat([df, df_latent_context], axis=1)\n",
    "elif method == 'AE': # latent context with AE\n",
    "    df_latent_context, latent_context_labels = get_latent_context_AE()\n",
    "    df = pd.concat([df, df_latent_context], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi6Hnk5grCYQ"
   },
   "source": [
    "### Hierarchical context extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pK0q24YHrQmo",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def is_leaf(node):\n",
    "    return node.left is None and node.right is None\n",
    "\n",
    "# Recursive function to find paths from root node to every leaf node of a binary tree\n",
    "def root_leaf_paths(node, path, hierarchy):\n",
    "\n",
    "    if node is None:\n",
    "        return\n",
    " \n",
    "    path.append(node.id)\n",
    " \n",
    "    if is_leaf(node):\n",
    "        hierarchy.append(list(path)) # append a complete path to the list of all paths\n",
    " \n",
    "    # Call the functions on left and right subtrees\n",
    "    root_leaf_paths(node.left, path, hierarchy)\n",
    "    root_leaf_paths(node.right, path, hierarchy)\n",
    " \n",
    "    # remove current node after left and right subtrees are done\n",
    "    path.pop()\n",
    "\n",
    "def hierarchical_clustering(df):\n",
    "    linked = linkage(df, 'ward')  # linkage matrix\n",
    "    rootnode, nodelist = to_tree(linked, rd=True) # tree representing the hierarchical clustering\n",
    "    path = deque() # a path from the root node to a leaf\n",
    "    hierarchy = []\n",
    "    root_leaf_paths(rootnode, path, hierarchy)\n",
    "    longest_path = len(max(hierarchy, key=len)) # find longest path from root to leaf\n",
    "    hierarchy = [x + [x[-1]]*(longest_path - len(x)) for x in hierarchy] # make path of equal size\n",
    "    hierarchy.sort(key=lambda x: x[-1]) # sort the list by the last element (datapoints id)\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-t8S6hDe9Jo",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hier_context = hierarchical_clustering(df.loc[:30, latent_context_labels])\n",
    "# hier_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvcSAyPmfgtc"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRlRnW0w6lpo",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary that contain evaluation metrics for each model\n",
    "models_eval_metrics = {}\n",
    "\n",
    "# embedding vectors length\n",
    "n_latent_factors_user = n_users // 1000\n",
    "n_latent_factors_item = n_items // 1000\n",
    "\n",
    "# latent factors for matrix factorization\n",
    "n_latent_factors_mf = n_items // 1000\n",
    "\n",
    "f'latent factor user: {n_latent_factors_user}  latent factor item: {n_latent_factors_item}  latent factor MF: {n_latent_factors_mf}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykDB1wk2y6ku"
   },
   "source": [
    "### Matrix factorization\n",
    "The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzxFuYa2zAW6",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "data = Dataset.load_from_df(df[x_labels + [y_label]], reader) # load df in surprise\n",
    "svd = SVD() # MF model\n",
    "result = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=2, verbose=True) # get result\n",
    "rmse = np.mean(result['test_rmse'])\n",
    "mae = np.mean(result['test_mae'])\n",
    "models_eval_metrics['MF'] = [rmse, mae]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REozrj4XYyD9"
   },
   "source": [
    "### NCF\n",
    "Multi-layer perceptron without context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz9i1ATFY6aS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def base_ncf(n_users, n_items, n_latent_factors_user, n_latent_factors_item):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "\n",
    "    # Item embedding\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item')([item_vec_mlp, user_vec_mlp])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # Output\n",
    "    pred_mlp = Dense(1, activation='relu', name='Activation')(dense_3)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input], pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AulSy1Ear21",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ncf = base_ncf(n_users, n_items, n_latent_factors_user, n_latent_factors_item)\n",
    "opt = keras.optimizers.Adam(lr = 0.0005)\n",
    "ncf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
    "\n",
    "# tf.keras.utils.plot_model(ncf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOKOMSjxb9Km",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse, mae = kfold_train(ncf, 15, 128, False, df, x_labels, y_label, kf, using_context=False)\n",
    "models_eval_metrics['NCF'] = [rmse, mae]\n",
    "f'k-fold RMSE = {rmse}     k-fold MAE = {mae}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9v4gahGImJG"
   },
   "source": [
    "### NeuMF\n",
    "Multi-layer perceptron + dot product without context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvvU8JM5IzAF",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def base_neumf(n_users, n_items, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "\n",
    "    # item embedding MF\n",
    "    item_embedding_mf = Embedding(n_items + 1, n_latent_factors_mf, name='item_embedding_MF')(item_input)\n",
    "    item_vec_mf = Flatten(name='flatten_item_MF')(item_embedding_mf)\n",
    "\n",
    "    # User embedding MF\n",
    "    user_embedding_mf = Embedding(n_users + 1, n_latent_factors_mf,name='user_embedding_MF')(user_input)\n",
    "    user_vec_mf = Flatten(name='flatten_user_MF')(user_embedding_mf)\n",
    "\n",
    "    # Dot product MF\n",
    "    dot = tf.keras.layers.Dot(axes=1)([user_vec_mf, item_vec_mf])\n",
    "\n",
    "    # Item embedding MLP\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding_MLP')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item_MLP')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding MLP\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding_MLP')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user_MLP')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item_context_MLP')([item_vec_mlp, user_vec_mlp])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # concat MF and MLP\n",
    "    concat_mf_mlp = Concatenate(name='MF_MLP')([dense_3, dot])\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, activation='relu',name='Activation')(concat_mf_mlp)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm0Uul47LuFG",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "neumf = base_neumf(n_users, n_items, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf)\n",
    "opt = keras.optimizers.Adam(lr = 0.0005)\n",
    "neumf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5ILww0IMy98",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse, mae = kfold_train(neumf, 15, 128, False, df, x_labels, y_label, kf, using_context=False)\n",
    "models_eval_metrics['NEUMF'] = [rmse, mae]\n",
    "f'k-fold RMSE = {rmse}     k-fold MAE = {mae}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cozjpFFyxM-"
   },
   "source": [
    "### ECAM NCF\n",
    "Multi-layer perceptron with explicit context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmSrKokyyxM-",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ncf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "    context_input = Input(shape=(n_context, ), name='context')\n",
    "\n",
    "    # Item embedding\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item')([item_vec_mlp, user_vec_mlp, context_input])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # Output\n",
    "    pred_mlp = Dense(1, activation='relu', name='Activation')(dense_3)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input, context_input], pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQKkj6_LyxM_",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ecam_ncf = ncf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item)\n",
    "opt = keras.optimizers.Adam(lr = 0.0005)\n",
    "ecam_ncf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
    "\n",
    "#ecam_ncf.summary()\n",
    "#tf.keras.utils.plot_model(ecam_ncf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJGY7tzAz5Tj",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse, mae = kfold_train(ecam_ncf, 15, 128, False, df, x_labels, y_label, kf, using_context=True, context_labels=context_labels)\n",
    "models_eval_metrics['ECAM NCF'] = [rmse, mae]\n",
    "f'k-fold RMSE = {rmse}     k-fold MAE = {mae}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBf_rCHlBnes"
   },
   "source": [
    "### ECAM NeuMF\n",
    "Multi-layer perceptron + dot product with explicit context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z_cfe0bBmUz",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def neumf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "    context_input = Input(shape=(n_context, ), name='context')\n",
    "\n",
    "    # item embedding MF\n",
    "    item_embedding_mf = Embedding(n_items + 1, n_latent_factors_mf, name='item_embedding_MF')(item_input)\n",
    "    item_vec_mf = Flatten(name='flatten_item_MF')(item_embedding_mf)\n",
    "\n",
    "    # User embedding MF\n",
    "    user_embedding_mf = Embedding(n_users + 1, n_latent_factors_mf,name='user_embedding_MF')(user_input)\n",
    "    user_vec_mf = Flatten(name='flatten_user_MF')(user_embedding_mf)\n",
    "\n",
    "    # Dot product MF\n",
    "    dot = tf.keras.layers.Dot(axes=1)([user_vec_mf, item_vec_mf])\n",
    "\n",
    "    # Item embedding MLP\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding_MLP')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item_MLP')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding MLP\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding_MLP')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user_MLP')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item_context_MLP')([item_vec_mlp, user_vec_mlp, context_input])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # concat MF and MLP\n",
    "    concat_mf_mlp = Concatenate(name='MF_MLP')([dense_3, dot])\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, activation='relu',name='Activation')(concat_mf_mlp)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input, context_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOK2BsrLB5nb",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ecam_neumf = neumf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf)\n",
    "opt = keras.optimizers.Adam(lr = 0.0005)\n",
    "ecam_neumf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
    "\n",
    "#ecam_neumf.summary()\n",
    "#tf.keras.utils.plot_model(ecam_neumf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQ7sbQdFqF0M",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse, mae = kfold_train(ecam_neumf, 15, 128, False, df, x_labels, y_label, kf, using_context=True, context_labels=context_labels)\n",
    "models_eval_metrics['ECAM NEUMF'] = [rmse, mae]\n",
    "f'k-fold RMSE = {rmse}     k-fold MAE = {mae}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okY4WYDkgNHz"
   },
   "source": [
    "### UCAM NCF\n",
    "Multi-layer perceptron with latent context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMqa52lDgHyc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ucam_ncf = ncf(n_users, n_items, n_latent_context, n_latent_factors_user, n_latent_factors_item)\n",
    "opt = keras.optimizers.Adam(lr = 0.005)\n",
    "ucam_ncf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
    "\n",
    "#ucam_ncf.summary()\n",
    "#tf.keras.utils.plot_model(ucam_ncf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQkIzbvHqT5c",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse, mae = kfold_train(ucam_ncf, 15, 128, False, df, x_labels, y_label, kf, \n",
    "                        using_context=True, context_labels=latent_context_labels)\n",
    "models_eval_metrics['UCAM NCF'] = [rmse, mae]\n",
    "f'k-fold RMSE = {rmse}     k-fold MAE = {mae}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx_QLRDs73oa"
   },
   "source": [
    "### UCAM NeuMF\n",
    "Multi-layer perceptron + dot product with latent context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmWmcZRR72RX",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ucam_neumf = neumf(n_users, n_items, n_latent_context, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf)\n",
    "opt = keras.optimizers.Adam(lr = 0.0005)\n",
    "ucam_neumf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI0qsmrKV8Bl",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse, mae = kfold_train(ucam_neumf, 15, 128, False, df, x_labels, y_label, kf, \n",
    "                        using_context=True, context_labels=latent_context_labels)\n",
    "models_eval_metrics['UCAM NEUMF'] = [rmse, mae]\n",
    "f'k-fold RMSE = {rmse}     k-fold MAE = {mae}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHQ2FEwlBdhq"
   },
   "source": [
    "## Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbOmETzaBhq3",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_models = len(models_eval_metrics) # number of different models\n",
    "models_name = [x[0] for x in models_eval_metrics.items()] \n",
    "rmse = [x[0] for x in models_eval_metrics.values()]\n",
    "mae = [x[1] for x in models_eval_metrics.values()]\n",
    "\n",
    "index = np.arange(n_models)\n",
    "bar_width = 0.30\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# MAE bar\n",
    "rect1 = plt.bar(index + bar_width, mae, bar_width,\n",
    "color='b',\n",
    "label='MAE')\n",
    "\n",
    "# RMSE bar\n",
    "rect2 = plt.bar(index, rmse, bar_width,\n",
    "color='#ff7b00',\n",
    "label='RMSE')\n",
    "\n",
    "plt.style.use('seaborn-ticks') # readable chart on dark editor\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Prediction results')\n",
    "plt.xticks(index + bar_width/2, models_name) # labels position\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('prediction_results.png')\n",
    "plt.show()\n",
    "\n",
    "for name, rmse, mae in zip(models_name, rmse, mae):\n",
    "    print(f\"Name: {name}      \\t      RMSE: {rmse}      \\t      MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YasL9GIGF3Jb",
    "IzbMVL48MAcU",
    "Hc-eF273f_OO",
    "OZPRR_K7yxM7",
    "NIfSxcyQzRzn",
    "P04efe0tHSi1",
    "Wi6Hnk5grCYQ",
    "ykDB1wk2y6ku",
    "REozrj4XYyD9",
    "Q9v4gahGImJG",
    "1cozjpFFyxM-",
    "IBf_rCHlBnes",
    "okY4WYDkgNHz",
    "Cx_QLRDs73oa"
   ],
   "name": "CARS_DL_Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
