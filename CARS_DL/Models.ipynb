{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7qy_0RayxMy"
   },
   "source": [
    "# CARS recommender system\n",
    "Implementation of the deep NN model described in the paper \"Context-Aware Recommendations Based on Deep\n",
    "Learning Frameworks\".\n",
    "https://dl.acm.org/doi/10.1145/3386243\n",
    "\n",
    "Datasets:\n",
    "- Frappe\n",
    "- Yelp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YasL9GIGF3Jb"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1q4UgOkhyxM4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # to split dataset in two parts\n",
    "from sklearn.model_selection import KFold # to split dataset using  k-fold cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import * # evaluation metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Input, Embedding, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # for creating chart\n",
    "import requests # for downloading the dataset\n",
    "from collections import deque # queue data structure\n",
    "from scipy.cluster.hierarchy import * # for hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection.validation import cross_validate\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzbMVL48MAcU"
   },
   "source": [
    "\n",
    "\n",
    "## Some functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CFVxgFQL_4P",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot loss based on history of model.fit, ymin and ymax are the minimum and maximum values of the y axis\n",
    "def plot_loss(history, ymin=0, ymax=1):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.ylim([ymin, ymax])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "# plot chart of true values on predictions\n",
    "def plot_predictions(test_y, pred_y):\n",
    "  a = plt.axes(aspect='equal')\n",
    "  plt.scatter(test_y, pred_y)\n",
    "  plt.xlabel('True Values')\n",
    "  plt.ylabel('Predictions')\n",
    "  lims = [0, 5]\n",
    "  plt.xlim(lims)\n",
    "  plt.ylim(lims)\n",
    "  _ = plt.plot(lims, lims)\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def kfold_gen(n_splits, df, xlabels, y_label, context_labels=[]):\n",
    "    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        train_x = df.loc[train_index, x_labels]  # get a dataset subset with df.loc[rows, columns]\n",
    "        train_y = df.loc[train_index, y_label]\n",
    "        test_x = df.loc[test_index, x_labels]\n",
    "        test_y = df.loc[test_index, y_label]\n",
    "        train_context = []\n",
    "        test_context = []\n",
    "        if context_labels: # if the context labels list is not empty\n",
    "            train_context = df.loc[train_index, context_labels]\n",
    "            test_context = df.loc[test_index, context_labels]\n",
    "        \n",
    "        yield train_x, train_y, test_x, test_y, train_context, test_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'yelp'\n",
    "\n",
    "if dataset == 'frappe':\n",
    "    df = pd.read_csv('final datasets/frappe_final.csv', sep=\",\")\n",
    "elif dataset == 'yelp':\n",
    "    df = pd.read_csv('final datasets/yelp_final2.csv', sep=\",\")\n",
    "    df = df[(df.groupby('user')['user'].transform('size') > 150) & (df.groupby('item')['item'].transform('size') > 150)]\n",
    "    df.user = pd.factorize(df.user)[0]\n",
    "    df.item = pd.factorize(df.item)[0]\n",
    "    df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = list(df.columns[0:2])\n",
    "y_label = df.columns[2]\n",
    "context_labels = list(df.columns[3:])\n",
    "# count number of unique users and items\n",
    "n_users, n_items = len(df.user.unique()), len(df.item.unique())\n",
    "n_context = len(context_labels)\n",
    "f'Number of users: {n_users}      Number of apps: {n_items}     Number of context features: {n_context}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIfSxcyQzRzn"
   },
   "source": [
    "### Latent context extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OclHf_U97Pr"
   },
   "source": [
    "#### With autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO1R_ZVl783U"
   },
   "outputs": [],
   "source": [
    "train_context_AE, test_context_AE = train_test_split(df.loc[:,context_labels], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iHJfOanzNQy"
   },
   "outputs": [],
   "source": [
    "# size of the encoded representation\n",
    "if dataset == 'frappe':\n",
    "    n_latent_context = 11\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    epochs = 50\n",
    "    batch_size = 128\n",
    "\n",
    "elif dataset == 'yelp':\n",
    "    n_latent_context = 3\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    epochs = 15\n",
    "    batch_size = 512\n",
    "\n",
    "\n",
    "# layer weight initializer\n",
    "weight_init = tf.keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
    "\n",
    "# input layer\n",
    "input = Input(shape=(n_context,))\n",
    "# the encoded representation of the input\n",
    "encoded = Dense(n_latent_context, activation='linear', kernel_initializer=weight_init)(input)\n",
    "# the reconstruction of the input\n",
    "decoded = Dense(n_context, activation='sigmoid', kernel_initializer=weight_init)(encoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = keras.Model(input, decoded)\n",
    "\n",
    "# Only encoder model\n",
    "encoder = keras.Model(input, encoded)\n",
    "\n",
    "# Only Decoder model\n",
    "encoded_input = keras.Input(shape=(n_latent_context,))   # takes as input the encoded context\n",
    "decoder_layer = autoencoder.layers[-1]   # Retrieve the last layer of the autoencoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer=opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqmXeqgOZ0KZ"
   },
   "outputs": [],
   "source": [
    "# train the autoencoder on the context\n",
    "history = autoencoder.fit(train_context_AE, train_context_AE,\n",
    "                          epochs=epochs,\n",
    "                          verbose=True,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(test_context_AE, test_context_AE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnPVHgFhOvzx"
   },
   "outputs": [],
   "source": [
    "plot_loss(history, ymin=0, ymax=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDXtf6_aEm_S"
   },
   "outputs": [],
   "source": [
    "# weight matrix of neurons that connect input layers to hidden layer\n",
    "# get weight returns a list of weights and biases, by taking weight[0] you extract only the weights\n",
    "weight_matrix = autoencoder.layers[1].get_weights()[0]\n",
    "weight_matrix = np.asarray(weight_matrix)\n",
    "\n",
    "def get_latent_context_AE():\n",
    "    latent_context = np.empty(shape=(df.shape[0], n_latent_context))\n",
    "    latent_context_labels = [f\"latent_{x}\" for x in range(n_latent_context)]\n",
    "    \n",
    "    # multiply each context sample for the weight matrix\n",
    "    for idx, s in enumerate(df.loc[:, context_labels].values):\n",
    "        latent_context[idx] = s @ weight_matrix\n",
    "    \n",
    "    # apply activation function\n",
    "    latent_context = sigmoid(latent_context)\n",
    "    df_latent_context = pd.DataFrame(latent_context, columns=latent_context_labels)\n",
    "\n",
    "    return df_latent_context, latent_context_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV89AtG_-Ms5"
   },
   "source": [
    "#### With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQiYQ_kJ-UoB",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_latent_context_PCA(n_latent_context):\n",
    "    latent_context_labels = [f\"latent_{x}\" for x in range(n_latent_context)]\n",
    "    pca = PCA(n_components=n_latent_context)\n",
    "    pca.fit(df.loc[:,context_labels])\n",
    "    latent_context = pca.transform(df.loc[:,context_labels])  \n",
    "    df_latent_context = pd.DataFrame(latent_context, columns=latent_context_labels)\n",
    "    return df_latent_context, latent_context_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P04efe0tHSi1"
   },
   "source": [
    "#### Run selected method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGbX4jMLHgzQ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "method = 'AE'\n",
    "if method == 'PCA': # latent context with PCA\n",
    "    df_latent_context, latent_context_labels = get_latent_context_PCA(n_latent_context)\n",
    "    df = pd.concat([df, df_latent_context], axis=1)\n",
    "elif method == 'AE': # latent context with AE\n",
    "    df_latent_context, latent_context_labels = get_latent_context_AE()\n",
    "    df = pd.concat([df, df_latent_context], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi6Hnk5grCYQ"
   },
   "source": [
    "### Hierarchical context extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pK0q24YHrQmo",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def is_leaf(node):\n",
    "    return node.left is None and node.right is None\n",
    "\n",
    "# Recursive function to find paths from root node to every leaf node of a binary tree\n",
    "def root_leaf_paths(node, path, hierarchy):\n",
    "\n",
    "    if node is None:\n",
    "        return\n",
    " \n",
    "    path.append(node.id)\n",
    " \n",
    "    if is_leaf(node):\n",
    "        hierarchy.append(list(path)) # append a complete path to the list of all paths\n",
    " \n",
    "    # Call the functions on left and right subtrees\n",
    "    root_leaf_paths(node.left, path, hierarchy)\n",
    "    root_leaf_paths(node.right, path, hierarchy)\n",
    " \n",
    "    # remove current node after left and right subtrees are done\n",
    "    path.pop()\n",
    "\n",
    "def hierarchical_clustering(df):\n",
    "    linked = linkage(df, 'ward')  # linkage matrix\n",
    "    rootnode, nodelist = to_tree(linked, rd=True) # tree representing the hierarchical clustering\n",
    "    path = deque() # a path from the root node to a leaf\n",
    "    hierarchy = []\n",
    "    root_leaf_paths(rootnode, path, hierarchy)\n",
    "    longest_path = len(max(hierarchy, key=len)) # find longest path from root to leaf\n",
    "    hierarchy = [x + [x[-1]]*(longest_path - len(x)) for x in hierarchy] # make path of equal size\n",
    "    hierarchy.sort(key=lambda x: x[-1]) # sort the list by the last element (datapoints id)\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-t8S6hDe9Jo",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hier_context = hierarchical_clustering(df.loc[:30, latent_context_labels])\n",
    "# hier_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvcSAyPmfgtc"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRlRnW0w6lpo"
   },
   "outputs": [],
   "source": [
    "# Dictionary that contain evaluation metrics for each model\n",
    "models_eval_metrics = {}\n",
    "\n",
    "if dataset == 'frappe':\n",
    "    # embedding vectors length\n",
    "    n_latent_factors_user = 10\n",
    "    n_latent_factors_item = 15\n",
    "    \n",
    "    # latent factors for matrix factorization\n",
    "    n_latent_factors_mf = 15\n",
    "    \n",
    "    # models hyperparameters\n",
    "    epochs = 12\n",
    "    batch_size = 256\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "elif dataset == 'yelp':\n",
    "    # embedding vectors length\n",
    "    n_latent_factors_user = 15\n",
    "    n_latent_factors_item = 20\n",
    "    \n",
    "    # latent factors for matrix factorization\n",
    "    n_latent_factors_mf = 20\n",
    "    \n",
    "    # models hyperparameters\n",
    "    epochs = 3\n",
    "    batch_size = 256\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    \n",
    "\n",
    "loss = 'mean_squared_error'\n",
    "n_splits = 5 # indicates how many parts the dataset is divided into\n",
    "\n",
    "f'latent factor user: {n_latent_factors_user}  latent factor item: {n_latent_factors_item}  latent factor MF: {n_latent_factors_mf}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykDB1wk2y6ku"
   },
   "source": [
    "### Matrix factorization\n",
    "The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzxFuYa2zAW6",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "data = Dataset.load_from_df(df[x_labels + [y_label]], reader) # load df in surprise\n",
    "svd = SVD(n_factors=20) # MF model\n",
    "result = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=n_splits, verbose=True)\n",
    "rmse = np.mean(result['test_rmse'])\n",
    "mae = np.mean(result['test_mae'])\n",
    "models_eval_metrics['MF'] = [rmse, mae]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REozrj4XYyD9"
   },
   "source": [
    "### NCF\n",
    "Multi-layer perceptron without context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz9i1ATFY6aS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def base_ncf(n_users, n_items, n_latent_factors_user, n_latent_factors_item):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "\n",
    "    # Item embedding\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item')([item_vec_mlp, user_vec_mlp])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # Output\n",
    "    pred_mlp = Dense(1, activation='relu', name='Activation')(dense_3)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input], pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = kfold_gen(n_splits, df, x_labels, y_label)\n",
    "rmse = np.empty(n_splits)\n",
    "mae = np.empty(n_splits)\n",
    "\n",
    "for idx in range(n_splits):\n",
    "    ncf = base_ncf(n_users, n_items, n_latent_factors_user, n_latent_factors_item)\n",
    "    ncf.compile(optimizer=opt, loss=loss, metrics=['mae', 'mse'])\n",
    "    train_x, train_y, test_x, test_y, train_context, test_context = next(kfold)\n",
    "    ncf.fit([train_x.user, train_x.item], train_y, epochs=epochs, batch_size=batch_size, verbose=False) # train\n",
    "    pred_y = ncf.predict([test_x.user, test_x.item]).flatten() # predict \n",
    "    rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "    mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "    print(f'Fold {idx}: RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "    \n",
    "models_eval_metrics['NCF'] = [np.mean(rmse), np.mean(mae)]\n",
    "print(models_eval_metrics['NCF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9v4gahGImJG"
   },
   "source": [
    "### NeuMF\n",
    "Multi-layer perceptron + dot product without context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvvU8JM5IzAF",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def base_neumf(n_users, n_items, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "\n",
    "    # item embedding MF\n",
    "    item_embedding_mf = Embedding(n_items + 1, n_latent_factors_mf, name='item_embedding_MF')(item_input)\n",
    "    item_vec_mf = Flatten(name='flatten_item_MF')(item_embedding_mf)\n",
    "\n",
    "    # User embedding MF\n",
    "    user_embedding_mf = Embedding(n_users + 1, n_latent_factors_mf,name='user_embedding_MF')(user_input)\n",
    "    user_vec_mf = Flatten(name='flatten_user_MF')(user_embedding_mf)\n",
    "\n",
    "    # Dot product MF\n",
    "    dot = tf.keras.layers.Dot(axes=1)([user_vec_mf, item_vec_mf])\n",
    "\n",
    "    # Item embedding MLP\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding_MLP')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item_MLP')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding MLP\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding_MLP')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user_MLP')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item_context_MLP')([item_vec_mlp, user_vec_mlp])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # concat MF and MLP\n",
    "    concat_mf_mlp = Concatenate(name='MF_MLP')([dense_3, dot])\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, activation='relu',name='Activation')(concat_mf_mlp)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = kfold_gen(n_splits, df, x_labels, y_label)\n",
    "rmse = np.empty(n_splits)\n",
    "mae = np.empty(n_splits)\n",
    "\n",
    "for idx in range(n_splits):\n",
    "    neumf = base_neumf(n_users, n_items, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf)\n",
    "    neumf.compile(optimizer=opt, loss=loss, metrics=['mae', 'mse'])\n",
    "    train_x, train_y, test_x, test_y, train_context, test_context = next(kfold)\n",
    "    neumf.fit([train_x.user, train_x.item], train_y, epochs=epochs, batch_size=batch_size, verbose=False) # train\n",
    "    pred_y = neumf.predict([test_x.user, test_x.item]).flatten() # predict \n",
    "    rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "    mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "    print(f'Fold {idx}: RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "    \n",
    "models_eval_metrics['NEUMF'] = [np.mean(rmse), np.mean(mae)]\n",
    "print(models_eval_metrics['NEUMF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cozjpFFyxM-"
   },
   "source": [
    "### ECAM NCF\n",
    "Multi-layer perceptron with explicit context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmSrKokyyxM-",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ncf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "    context_input = Input(shape=(n_context, ), name='context')\n",
    "\n",
    "    # Item embedding\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item')([item_vec_mlp, user_vec_mlp, context_input])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # Output\n",
    "    pred_mlp = Dense(1, activation='relu', name='Activation')(dense_3)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input, context_input], pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = kfold_gen(n_splits, df, x_labels, y_label, context_labels)\n",
    "rmse = np.empty(n_splits)\n",
    "mae = np.empty(n_splits)\n",
    "\n",
    "for idx in range(n_splits):\n",
    "    ecam_ncf = ncf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item)\n",
    "    ecam_ncf.compile(optimizer=opt, loss=loss, metrics=['mae', 'mse'])\n",
    "    train_x, train_y, test_x, test_y, train_context, test_context = next(kfold)\n",
    "    ecam_ncf.fit([train_x.user, train_x.item, train_context], train_y, epochs=epochs, batch_size=batch_size, verbose=False) # train\n",
    "    pred_y = ecam_ncf.predict([test_x.user, test_x.item, test_context]).flatten() # predict \n",
    "    rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "    mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "    print(f'Fold {idx}: RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "    \n",
    "models_eval_metrics['ECAM NCF'] = [np.mean(rmse), np.mean(mae)]\n",
    "print(models_eval_metrics['ECAM NCF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okY4WYDkgNHz"
   },
   "source": [
    "### UCAM NCF\n",
    "Multi-layer perceptron with latent context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = kfold_gen(n_splits, df, x_labels, y_label, latent_context_labels)\n",
    "rmse = np.empty(n_splits)\n",
    "mae = np.empty(n_splits)\n",
    "\n",
    "for idx in range(n_splits):\n",
    "    ucam_ncf = ncf(n_users, n_items, n_latent_context, n_latent_factors_user, n_latent_factors_item)\n",
    "    ucam_ncf.compile(optimizer=opt, loss=loss, metrics=['mae', 'mse'])\n",
    "    train_x, train_y, test_x, test_y, train_context, test_context = next(kfold)\n",
    "    ucam_ncf.fit([train_x.user, train_x.item, train_context], train_y, epochs=epochs, batch_size=batch_size, verbose=False) # train\n",
    "    pred_y = ucam_ncf.predict([test_x.user, test_x.item, test_context]).flatten() # predict \n",
    "    rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "    mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "    print(f'Fold {idx}: RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "\n",
    "models_eval_metrics['UCAM NCF'] = [np.mean(rmse), np.mean(mae)]\n",
    "print(models_eval_metrics['UCAM NCF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBf_rCHlBnes"
   },
   "source": [
    "### ECAM NeuMF\n",
    "Multi-layer perceptron + dot product with explicit context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z_cfe0bBmUz",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def neumf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf):\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "    context_input = Input(shape=(n_context, ), name='context')\n",
    "\n",
    "    # item embedding MF\n",
    "    item_embedding_mf = Embedding(n_items + 1, n_latent_factors_mf, name='item_embedding_MF')(item_input)\n",
    "    item_vec_mf = Flatten(name='flatten_item_MF')(item_embedding_mf)\n",
    "\n",
    "    # User embedding MF\n",
    "    user_embedding_mf = Embedding(n_users + 1, n_latent_factors_mf,name='user_embedding_MF')(user_input)\n",
    "    user_vec_mf = Flatten(name='flatten_user_MF')(user_embedding_mf)\n",
    "\n",
    "    # Dot product MF\n",
    "    dot = tf.keras.layers.Dot(axes=1)([user_vec_mf, item_vec_mf])\n",
    "\n",
    "    # Item embedding MLP\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding_MLP')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item_MLP')(item_embedding_mlp)\n",
    "\n",
    "    # User embedding MLP\n",
    "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding_MLP')(user_input)\n",
    "    user_vec_mlp = Flatten(name='flatten_user_MLP')(user_embedding_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item_context_MLP')([item_vec_mlp, user_vec_mlp, context_input])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(dense)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(dense_2)\n",
    "\n",
    "    # concat MF and MLP\n",
    "    concat_mf_mlp = Concatenate(name='MF_MLP')([dense_3, dot])\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, activation='relu',name='Activation')(concat_mf_mlp)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input, context_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = kfold_gen(n_splits, df, x_labels, y_label, context_labels)\n",
    "rmse = np.empty(n_splits)\n",
    "mae = np.empty(n_splits)\n",
    "\n",
    "for idx in range(n_splits):\n",
    "    ecam_neumf = neumf(n_users, n_items, n_context, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf)\n",
    "    ecam_neumf.compile(optimizer=opt, loss=loss, metrics=['mae', 'mse'])\n",
    "    train_x, train_y, test_x, test_y, train_context, test_context = next(kfold)\n",
    "    ecam_neumf.fit([train_x.user, train_x.item, train_context], train_y, epochs=5, batch_size=256, verbose=False) # train\n",
    "    pred_y = ecam_neumf.predict([test_x.user, test_x.item, test_context]).flatten() # predict \n",
    "    rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "    mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "    print(f'Fold {idx}: RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "    \n",
    "models_eval_metrics['ECAM NEUMF'] = [np.mean(rmse), np.mean(mae)]\n",
    "print(models_eval_metrics['ECAM NEUMF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx_QLRDs73oa"
   },
   "source": [
    "### UCAM NeuMF\n",
    "Multi-layer perceptron + dot product with latent context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = kfold_gen(n_splits, df, x_labels, y_label, latent_context_labels)\n",
    "rmse = np.empty(n_splits)\n",
    "mae = np.empty(n_splits)\n",
    "\n",
    "for idx in range(1):\n",
    "    ucam_neumf = neumf(n_users, n_items, n_latent_context, n_latent_factors_user, n_latent_factors_item, n_latent_factors_mf)\n",
    "    ucam_neumf.compile(optimizer=opt, loss=loss, metrics=['mae', 'mse'])    \n",
    "    train_x, train_y, test_x, test_y, train_context, test_context = next(kfold)\n",
    "    history = ucam_neumf.fit([train_x.user, train_x.item, train_context], train_y, epochs=epochs, batch_size=batch_size, verbose=False) # train\n",
    "    pred_y = ucam_neumf.predict([test_x.user, test_x.item, test_context]).flatten() # predict \n",
    "    rmse[idx] = mean_squared_error(test_y, pred_y, squared = False)\n",
    "    mae[idx] = mean_absolute_error(test_y, pred_y)\n",
    "    print(f'Fold {idx}: RMSE = {rmse[idx]}    MAE = {mae[idx]}')\n",
    "    plot_loss(history, ymin=0, ymax=1)\n",
    "    \n",
    "models_eval_metrics['UCAM NEUMF'] = [np.mean(rmse), np.mean(mae)]\n",
    "print(models_eval_metrics['UCAM NEUMF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHQ2FEwlBdhq"
   },
   "source": [
    "## Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbOmETzaBhq3",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_models = len(models_eval_metrics) # number of different models\n",
    "models_name = [x[0] for x in models_eval_metrics.items()] \n",
    "rmse = [x[0] for x in models_eval_metrics.values()]\n",
    "mae = [x[1] for x in models_eval_metrics.values()]\n",
    "\n",
    "index = np.arange(n_models)\n",
    "bar_width = 0.30\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# MAE bar\n",
    "rect1 = plt.bar(index + bar_width, mae, bar_width,\n",
    "color='b',\n",
    "label='MAE')\n",
    "\n",
    "# RMSE bar\n",
    "rect2 = plt.bar(index, rmse, bar_width,\n",
    "color='#ff7b00',\n",
    "label='RMSE')\n",
    "\n",
    "plt.style.use('seaborn-ticks') # readable chart on dark editor\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Prediction results')\n",
    "plt.xticks(index + bar_width/2, models_name) # labels position\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('prediction_results.png')\n",
    "plt.show()\n",
    "\n",
    "for name, rmse, mae in zip(models_name, rmse, mae):\n",
    "    print(f\"Name: {name}      \\t      RMSE: {rmse}      \\t      MAE: {mae}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YasL9GIGF3Jb",
    "IzbMVL48MAcU",
    "Hc-eF273f_OO",
    "OZPRR_K7yxM7",
    "NIfSxcyQzRzn",
    "P04efe0tHSi1",
    "Wi6Hnk5grCYQ",
    "ykDB1wk2y6ku",
    "REozrj4XYyD9",
    "Q9v4gahGImJG",
    "1cozjpFFyxM-",
    "IBf_rCHlBnes",
    "okY4WYDkgNHz",
    "Cx_QLRDs73oa"
   ],
   "name": "CARS_DL_Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
