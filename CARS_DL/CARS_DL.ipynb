{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "Copia di CARS_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YasL9GIGF3Jb",
        "Q1m4VXWTFeGE",
        "1cozjpFFyxM-",
        "IBf_rCHlBnes"
      ]
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7qy_0RayxMy"
      },
      "source": [
        "# CARS recommender system\n",
        "Implementation of the deep NN model described in the paper \"Context-Aware Recommendations Based on Deep\n",
        "Learning Frameworks\".\n",
        "https://dl.acm.org/doi/10.1145/3386243\n",
        "\n",
        "Datasets:\n",
        "- frappe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YasL9GIGF3Jb"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPMS2KCF0nl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q4UgOkhyxM4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split # to split dataset\n",
        "from sklearn.metrics import * # evaluation metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt # for creating chart\n",
        "import requests # for downloading the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1m4VXWTFeGE"
      },
      "source": [
        "## Load and display dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxz5jCSiyxM6"
      },
      "source": [
        "# download the dataset\n",
        "url = 'https://raw.githubusercontent.com/CriptHunter/tesi/master/CARS_DL/frappe/frappe.csv'\n",
        "req = requests.get(url, allow_redirects=True)\n",
        "open('frappe.csv', 'wb').write(req.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlzGyC-g6HQM"
      },
      "source": [
        "# open the dataset\n",
        "df = pd.read_csv('/content/frappe.csv', sep=\"\\t\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoRlfX6OyxM7"
      },
      "source": [
        "# count unique values for each column\n",
        "display(\"------ unique values ------\")\n",
        "display(df.nunique())\n",
        "\n",
        "# count number of unknown values for each column\n",
        "display(\"------ unknown values ------\")\n",
        "display(df.isin(['unknown']).sum(axis=0))\n",
        "\n",
        "# count number of zero values for each column (for city 0 == unknown)\n",
        "display(\"------ zero values ------\")\n",
        "display(df.isin([0]).sum(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZPRR_K7yxM7"
      },
      "source": [
        "## Dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6_OSazcyxM8"
      },
      "source": [
        "# log transformation on the raw frequency numbers that represent the applications usage\n",
        "df['cnt'] = df['cnt'].apply(np.log10)\n",
        "f\"frequency range is {df['cnt'][df['cnt'] == df['cnt'].min()].values[0]} to {df['cnt'][df['cnt'] == df['cnt'].max()].values[0]}\"\n",
        "\n",
        "# delete columns that are not needed\n",
        "del df['homework']\n",
        "del df['cost']\n",
        "del df['city']\n",
        "del df['isweekend']\n",
        "del df['country']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFAc_KxJyxM8"
      },
      "source": [
        "# delete rows where weather is unknown\n",
        "df = df[df.weather != 'unknown']\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# make user and items id start from 0\n",
        "df.user = pd.factorize(df.user)[0]\n",
        "df.item = pd.factorize(df.item)[0]\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzwi3oscyxM8"
      },
      "source": [
        "context_labels = ['daytime', 'weekday', 'weather']\n",
        "\n",
        "# convert categorical data to one-hot encoding\n",
        "for col in context_labels:\n",
        "  df = pd.get_dummies(df, columns=[col], prefix = [col])\n",
        "\n",
        "# new context labels after one-hot encoding are columns from 3 to the end\n",
        "context_labels = df.columns[3:]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njoz95J4yxM9"
      },
      "source": [
        "# train and test datasets\n",
        "train_x, test_x = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# train and test context features\n",
        "train_context = pd.concat([train_x.pop(x) for x in context_labels], axis=1)\n",
        "test_context = pd.concat([test_x.pop(x) for x in context_labels], axis=1)\n",
        "\n",
        "# train and test values to predict\n",
        "train_y = train_x.pop('cnt')\n",
        "test_y = test_x.pop('cnt')\n",
        "\n",
        "f\"train_x: {train_x.shape}   train_y: {train_y.shape}   train_context: {train_context.shape}    test_x: {test_x.shape}   test_y: {test_y.shape}     test_context:   {test_context.shape}\"    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYutpX2ryxM-"
      },
      "source": [
        "# count number of unique users and items\n",
        "n_users, n_items = len(df.user.unique()), len(df.item.unique())\n",
        "n_context = len(context_labels)\n",
        "\n",
        "# embedding vectors length\n",
        "n_latent_factors_user = 8\n",
        "n_latent_factors_item = 12\n",
        "\n",
        "f'Number of users: {n_users}      Number of apps: {n_items}     Number of context features: {n_context}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cozjpFFyxM-"
      },
      "source": [
        "## ECAM NCF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmSrKokyyxM-"
      },
      "source": [
        "def ecam_ncf():\n",
        "    # inputs\n",
        "    item_input = Input(shape=[1],name='item')\n",
        "    user_input = Input(shape=[1],name='user')\n",
        "    context_input = Input(shape=(n_context, ), name='context')\n",
        "\n",
        "    # Item embedding\n",
        "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding')(item_input)\n",
        "    item_vec_mlp = Flatten(name='flatten_item')(item_embedding_mlp)\n",
        "    item_vec_mlp = Dropout(0.2)(item_vec_mlp)\n",
        "\n",
        "    # User embedding\n",
        "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding')(user_input)\n",
        "    user_vec_mlp = Flatten(name='flatten_user')(user_embedding_mlp)\n",
        "    user_vec_mlp = Dropout(0.2)(user_vec_mlp)\n",
        "\n",
        "    # Concat user embedding,item embeddings and context vector\n",
        "    concat = Concatenate(name='user_item')([item_vec_mlp, user_vec_mlp, context_input])\n",
        "\n",
        "    # dense layers\n",
        "    dense = Dense(8, name='fully_connected_1')(concat)\n",
        "    batch_1 = BatchNormalization()(dense)\n",
        "    dense_2 = Dense(4, name='fully_connected_2')(batch_1)\n",
        "    batch_2 = BatchNormalization()(dense_2)\n",
        "    dense_3 = Dense(2, name='fully_connected_3')(batch_2)\n",
        "\n",
        "    # Output\n",
        "    pred_mlp = Dense(1, activation='relu', name='Activation')(dense_3)\n",
        "\n",
        "    # make and build the model\n",
        "    return keras.Model([user_input, item_input, context_input], pred_mlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQKkj6_LyxM_"
      },
      "source": [
        "ecam_ncf = ecam_ncf()\n",
        "opt = keras.optimizers.Adam(lr = 0.005)\n",
        "ecam_ncf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
        "\n",
        "ecam_ncf.summary()\n",
        "tf.keras.utils.plot_model(ecam_ncf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga8vO66qyxM_"
      },
      "source": [
        "history = ecam_ncf.fit([train_x.user, train_x.item, train_context], train_y, epochs=15, batch_size=128, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9em8HFenyxNA"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.ylim([0, 1])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6DuplByxNA"
      },
      "source": [
        "# prediction on the test set\n",
        "pred_y = ecam_ncf.predict([test_x.user, test_x.item, test_context]).flatten()\n",
        "\n",
        "# chart that show predictions and true values\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_y, pred_y)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "lims = [0, 5]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib8whJ3zyxNB"
      },
      "source": [
        "# evaluation metrics on the test set\n",
        "rmse = mean_squared_error(test_y, pred_y, squared = False)\n",
        "mse = mean_squared_error(test_y, pred_y, squared = True)\n",
        "mae = mean_absolute_error(test_y, pred_y)\n",
        "f'RMSE = {rmse}    MAE = {mae}    MSE = {mse}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBf_rCHlBnes"
      },
      "source": [
        "## ECAM NeuMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOM7abQ7ExgO"
      },
      "source": [
        "# latent factors for matrix factorization\n",
        "n_latent_factors_mf = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z_cfe0bBmUz"
      },
      "source": [
        "def ecam_neumf():\n",
        "    # inputs\n",
        "    item_input = Input(shape=[1],name='item')\n",
        "    user_input = Input(shape=[1],name='user')\n",
        "    context_input = Input(shape=(n_context, ), name='context')\n",
        "\n",
        "    # item embedding MF\n",
        "    item_embedding_mf = Embedding(n_items + 1, n_latent_factors_mf, name='item_embedding_MF')(item_input)\n",
        "    item_vec_mf = Flatten(name='flatten_item_MF')(item_embedding_mf)\n",
        "    item_vec_mf = Dropout(0.2)(item_vec_mf)\n",
        "\n",
        "    # User embedding MF\n",
        "    user_embedding_mf = Embedding(n_users + 1, n_latent_factors_mf,name='user_embedding_MF')(user_input)\n",
        "    user_vec_mf = Flatten(name='flatten_user_MF')(user_embedding_mf)\n",
        "    user_vec_mf = Dropout(0.2)(user_vec_mf)\n",
        "\n",
        "    # Dot product MF\n",
        "    dot = tf.keras.layers.Dot(axes=1)([user_vec_mf, item_vec_mf])\n",
        "\n",
        "    # Item embedding MLP\n",
        "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors_item, name='item_embedding_MLP')(item_input)\n",
        "    item_vec_mlp = Flatten(name='flatten_item_MLP')(item_embedding_mlp)\n",
        "    item_vec_mlp = Dropout(0.2)(item_vec_mlp)\n",
        "\n",
        "    # User embedding MLP\n",
        "    user_embedding_mlp = Embedding(n_users + 1, n_latent_factors_user,name='user_embedding_MLP')(user_input)\n",
        "    user_vec_mlp = Flatten(name='flatten_user_MLP')(user_embedding_mlp)\n",
        "    user_vec_mlp = Dropout(0.2)(user_vec_mlp)\n",
        "\n",
        "    # Concat user embedding,item embeddings and context vector\n",
        "    concat = Concatenate(name='user_item_context_MLP')([item_vec_mlp, user_vec_mlp, context_input])\n",
        "\n",
        "    # dense layers\n",
        "    dense = Dense(8, name='fully_connected_1')(concat)\n",
        "    batch_1 = BatchNormalization()(dense)\n",
        "    dense_2 = Dense(4, name='fully_connected_2')(batch_1)\n",
        "    batch_2 = BatchNormalization()(dense_2)\n",
        "    dense_3 = Dense(2, name='fully_connected_3')(batch_2)\n",
        "\n",
        "    # concat MF and MLP\n",
        "    concat_mf_mlp = Concatenate(name='MF_MLP')([dense_3, dot])\n",
        "\n",
        "    # Output\n",
        "    output = Dense(1, activation='relu',name='Activation')(concat_mf_mlp)\n",
        "\n",
        "    # make and build the model\n",
        "    return keras.Model([user_input, item_input, context_input], output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOK2BsrLB5nb"
      },
      "source": [
        "ecam_neumf = ecam_neumf()\n",
        "opt = keras.optimizers.Adam(lr = 0.005)\n",
        "ecam_neumf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
        "\n",
        "ecam_neumf.summary()\n",
        "tf.keras.utils.plot_model(ecam_neumf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_R68XpIF9vr"
      },
      "source": [
        "history = ecam_neumf.fit([train_x.user, train_x.item, train_context], train_y, epochs=15, batch_size=128, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqMD7FpsGLzO"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvDgS2oUGOeX"
      },
      "source": [
        "pred_y = ecam_neumf.predict([test_x.user, test_x.item, test_context]).flatten()\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_y, pred_y)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "lims = [0, 5]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDcqB4OQGYrn"
      },
      "source": [
        "rmse = mean_squared_error(test_y, pred_y, squared = False)\n",
        "mse = mean_squared_error(test_y, pred_y, squared = True)\n",
        "mae = mean_absolute_error(test_y, pred_y)\n",
        "f'RMSE = {rmse}    MAE = {mae}    MSE = {mse}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIfSxcyQzRzn"
      },
      "source": [
        "## Latent context extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iHJfOanzNQy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}