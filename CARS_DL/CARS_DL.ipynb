{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CARS recommender system\n",
    "Implementation of the deep NN model described in the paper \"Context-Aware Recommendations Based on Deep\n",
    "Learning Frameworks\".\n",
    "https://dl.acm.org/doi/10.1145/3386243\n",
    "\n",
    "Datasets:\n",
    "- frappe\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # to split dataset\n",
    "from sklearn.metrics import * # evaluation metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt # for creating visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('frappe/frappe.csv', sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "source": [
    "## Dataset preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation on the raw frequency numbers represents the application usage\n",
    "df['cnt'] = df['cnt'].apply(np.log10)\n",
    "f\"frequency range is {df['cnt'][df['cnt'] == df['cnt'].min()].values[0]} to {df['cnt'][df['cnt'] == df['cnt'].max()].values[0]}\"\n",
    "\n",
    "# delete columns that are not needed\n",
    "del df['homework']\n",
    "del df['cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows where city and weather are unknown\n",
    "df = df[df.city != 0]\n",
    "df = df[df.weather != 'unknown']\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# make user and items id start from 0\n",
    "df.user = pd.factorize(df.user)[0]\n",
    "df.item = pd.factorize(df.item)[0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column type to category and convert categorical data to integers\n",
    "context_labels = ['daytime', 'weekday', 'isweekend', 'weather', 'country', 'city']\n",
    "for col in context_labels:\n",
    "    df[col] = df[col].astype('category').cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test datasets\n",
    "train_x, test_x = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# train and test context features\n",
    "train_context = pd.concat([train_x.pop(x) for x in context_labels], axis=1)\n",
    "test_context = pd.concat([test_x.pop(x) for x in context_labels], axis=1)\n",
    "\n",
    "# train and test labels\n",
    "train_y = train_x.pop('cnt')\n",
    "test_y = test_x.pop('cnt')\n",
    "\n",
    "f\"train_x: {train_x.shape}   train_y: {train_y.shape}   train_context: {train_context.shape}    test_x: {test_x.shape}   test_y: {test_y.shape}     test_context:   {test_context.shape}\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "train_context = min_max_norm(train_context)\n",
    "test_context = min_max_norm(test_context)\n",
    "train_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent_factors = 5\n",
    "\n",
    "# count unique user and item\n",
    "n_users, n_items = len(df.user.unique()), len(df.item.unique())\n",
    "n_context = len(context_labels)\n",
    "f'Number of users: {n_users}      Number of apps: {n_items}     Number of context features: {n_context}'"
   ]
  },
  {
   "source": [
    "## ECAM NCF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecam_ncf():\n",
    "    # inputs\n",
    "    item_input = Input(shape=[1],name='item')\n",
    "    user_input = Input(shape=[1],name='user')\n",
    "    context_input = Input(shape=(n_context, ), name='context')\n",
    "\n",
    "    # Item embedding\n",
    "    item_embedding_mlp = Embedding(n_items + 1, n_latent_factors, name='item_embedding')(item_input)\n",
    "    item_vec_mlp = Flatten(name='flatten_item')(item_embedding_mlp)\n",
    "    item_vec_mlp = keras.layers.Dropout(0.2)(item_vec_mlp)\n",
    "\n",
    "    # User embedding\n",
    "    user_vec_mlp = Flatten(name='flatten_user')(keras.layers.Embedding(n_users + 1, n_latent_factors,name='user_embedding')(user_input))\n",
    "    user_vec_mlp = keras.layers.Dropout(0.2)(user_vec_mlp)\n",
    "\n",
    "    # Concat user embedding,item embeddings and context vector\n",
    "    concat = Concatenate(name='user_item')([item_vec_mlp, user_vec_mlp, context_input])\n",
    "\n",
    "    # dense layers\n",
    "    dense = Dense(8, name='fully_connected_1')(concat)\n",
    "    batch_1 = BatchNormalization()(dense)\n",
    "    dense_2 = Dense(4, name='fully_connected_2')(batch_1)\n",
    "    batch_2 = BatchNormalization()(dense_2)\n",
    "    dense_3 = Dense(2, name='fully_connected_3')(batch_2)\n",
    "\n",
    "    # Output\n",
    "    pred_mlp = Dense(1, activation='relu',name='Activation')(dense_3)\n",
    "\n",
    "    # make and build the model\n",
    "    return keras.Model([user_input, item_input, context_input], pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecam_ncf = ecam_ncf()\n",
    "opt = keras.optimizers.Adam(lr = 0.005)\n",
    "ecam_ncf.compile(optimizer = opt,loss= 'mean_absolute_error', metrics=['mae', 'mse'])\n",
    "\n",
    "ecam_ncf.summary()\n",
    "tf.keras.utils.plot_model(ecam_ncf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ecam_ncf.fit([train_x.user, train_x.item, train_context], train_y, epochs=15, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.ylim([0, 1])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = ecam_ncf.predict([test_x.user, test_x.item, test_context]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_y, pred_y)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [0, 5]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(test_y, pred_y, squared = False)\n",
    "mse = mean_squared_error(test_y, pred_y, squared = True)\n",
    "mae = mean_absolute_error(test_y, pred_y)\n",
    "f'RMSE = {rmse}    MAE = {mae}    MSE = {mse}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}