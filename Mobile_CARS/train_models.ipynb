{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models MDF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Embedding, Flatten, Concatenate, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import rs_models\n",
    "import implicit\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from implicit.evaluation import AUC_at_k, precision_at_k, train_test_split\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating with value 1: 67.04773696519466 %\n",
      "users: 30 \t items: 338 \t rating: 72690 \t items_features: 26 \t contexts_features: 63 \t \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/MDF_final.csv')\n",
    "df = df.drop_duplicates()\n",
    "df.user = pd.factorize(df.user)[0] # make sure that user and item IDs start from zero\n",
    "df.item = pd.factorize(df.item)[0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# df = df.drop(['place_type_food_and_drink', 'place_type_health', 'place_type_home', 'place_type_lodging','place_type_outdoors', 'place_type_point_of_interest_establishment','place_type_public_transport_station', 'place_type_school','place_type_service', 'place_type_store', 'place_type_workplace'], axis = 1)\n",
    "\n",
    "item_labels = [i for i in list(df.columns) if i.find(\"category\") == 0] # labels that describe an item\n",
    "context_labels = list(set(df.iloc[:, 3:]) - set(item_labels)) # takes all the columns after user, item rating and remove item labels\n",
    "\n",
    "n_users = df.user.nunique()\n",
    "n_items = df.item.nunique()\n",
    "n_contexts = len(context_labels)\n",
    "\n",
    "print(f\"rating with value 1: {df[df.rating == 1]['rating'].count() * 100 / len(df)} %\")\n",
    "print(f\"users: {n_users} \\t items: {n_items} \\t rating: {len(df)} \\t items_features: {len(item_labels)} \\t contexts_features: {n_contexts} \\t \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "models_eval_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS matrix factorization\n",
    "- Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix factorization \t AUC@10: 0.722977254819739 \t precision@10: 0.43877750871007437\n"
     ]
    }
   ],
   "source": [
    "df_mf = pd.read_csv('Datasets/MDF_matrix_factorization.csv')\n",
    "k = 10\n",
    "auc, precision = rs_models.train_mf(df_mf, factors=128, regularization=5, iterations=10, n_splits=10, k=k)\n",
    "print(f'Matrix factorization \\t AUC@{k}: {auc} \\t precision@{k}: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'n_contexts': n_contexts,\n",
    "    'learn_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 10\n",
    "}   \n",
    "\n",
    "\n",
    "std_dev, accuracy, auc, precision, recall = rs_models.kfold_train(rs_models.NeuMF, param, df, n_splits=n_splits)\n",
    "models_eval_metrics['NeuMF'] = [accuracy, auc, precision, recall]\n",
    "print(f\"NeuMF \\t accuracy: {accuracy*100}% \\t AUC: {auc} \\t precision: {precision} \\t recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECAM NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'n_contexts': n_contexts,\n",
    "    'learn_rate': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 10\n",
    "}  \n",
    "\n",
    "std_dev, accuracy, auc, precision, recall = rs_models.kfold_train(rs_models.ECAM_NeuMF, param, df, context_labels=context_labels, n_splits=n_splits)\n",
    "models_eval_metrics['ECAM NeuMF'] = [accuracy, auc, precision, recall]\n",
    "print(f\"ECAM NeuMF \\t accuracy: {accuracy*100}% \\t AUC: {auc} \\t precision: {precision} \\t recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "ffnet = KerasClassifier(build_fn=rs_models.mobile_model, neurons=100, layers=3, learn_rate=0.005, epochs=10, batch_size=64, verbose=False)\n",
    "x = df[item_labels+context_labels]\n",
    "y = df['rating']\n",
    "\n",
    "scores = cross_validate(ffnet, x, y, cv=KFold(shuffle=True, n_splits=n_splits, random_state=42), scoring=['accuracy', 'roc_auc', 'precision', 'recall'])\n",
    "\n",
    "accuracy = np.average(scores['test_accuracy'])\n",
    "auc = np.average(scores['test_roc_auc'])\n",
    "precision = np.average(scores['test_precision'])\n",
    "recall = np.average(scores['test_recall'])\n",
    "models_eval_metrics['Classifier'] = [accuracy, auc, precision, recall]\n",
    "\n",
    "print(f\"Classifier \\t accuracy: {accuracy*100}% \\t AUC: {auc} \\t precision: {precision} \\t recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = len(models_eval_metrics) # number of different models\n",
    "models_name = [x[0] for x in models_eval_metrics.items()] \n",
    "\n",
    "accuracy = [x[0] for x in models_eval_metrics.values()]\n",
    "auc = [x[1] for x in models_eval_metrics.values()]\n",
    "precision = [x[2] for x in models_eval_metrics.values()]\n",
    "recall = [x[3] for x in models_eval_metrics.values()]\n",
    "\n",
    "index = np.arange(n_models)\n",
    "bar_width = 0.15\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# AUC bar\n",
    "rect1 = plt.bar(index + bar_width, auc, bar_width, color='b', label='AUC')\n",
    "\n",
    "# accuracy bar\n",
    "rect2 = plt.bar(index, accuracy, bar_width, color='#ff7b00', label='Accuracy')\n",
    "\n",
    "# precision bar\n",
    "rect3 = plt.bar(index + bar_width*2, precision, bar_width, color='g', label='Precision')\n",
    "\n",
    "# recall bar\n",
    "rect2 = plt.bar(index + bar_width*3, recall, bar_width, color='r', label='Recall')\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Prediction results')\n",
    "plt.xticks(index + bar_width * 3/2, models_name) # labels position\n",
    "plt.legend(bbox_to_anchor=(1.3, 1))\n",
    "plt.grid(True)\n",
    "plt.savefig('prediction_results.png')\n",
    "plt.show()\n",
    "\n",
    "for key, item in models_eval_metrics.items():\n",
    "    print(f'Model: {key} \\t Accuracy: {item[0]*100}% \\t AUC: {item[1]} \\t Precision: {item[2]} \\t Recall: {item[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
