{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Embedding, Flatten, Concatenate, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "import rs_models\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import implicit\n",
    "from implicit.evaluation import AUC_at_k, precision_at_k, train_test_split\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'frappe'\n",
    "\n",
    "if dataset == 'mdf':\n",
    "    df = pd.read_csv('Datasets/MDF_final.csv')\n",
    "    df_mf = pd.read_csv('Datasets/MDF_matrix_factorization.csv')\n",
    "    df = df.drop(columns='time')\n",
    "    df = df.drop_duplicates()\n",
    "    # df = df[df.item != 2]\n",
    "    # df = df.drop(['place_type_food_and_drink', 'place_type_health', 'place_type_home', 'place_type_lodging','place_type_outdoors', 'place_type_point_of_interest_establishment','place_type_public_transport_station', 'place_type_school','place_type_service', 'place_type_store', 'place_type_workplace'], axis = 1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    context_labels = list(df.columns[3:66])\n",
    "    item_labels = list(df.columns[66:92])\n",
    "    user_labels = list(df.columns[92:])\n",
    "\n",
    "elif dataset == 'frappe':\n",
    "    df = pd.read_csv('Datasets/frappe dataset/frappe_final.csv')\n",
    "    df_mf = pd.read_csv('Datasets/frappe dataset/frappe_matrix_factorization.csv')\n",
    "    item_labels = list(df.columns[27:54])\n",
    "    user_labels = list(df.columns[54:])\n",
    "    context_labels = list(df.columns[3:27])\n",
    "    \n",
    "elif dataset == 'yelp':\n",
    "    df = pd.read_csv('Datasets/yelp dataset/yelp_final.csv')\n",
    "    df_mf = pd.read_csv('Datasets/yelp dataset/yelp_matrix_factorization.csv')\n",
    "    item_labels = list(df.columns[18:488])\n",
    "    user_labels = list(df.columns[488:])\n",
    "    context_labels = list(df.columns[3:18])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating with value 1: 62.04633943958639 %\n",
      "users: 857 \t items: 3180 \t rating: 78335 \t user_features: 47 \t items_features: 27 \t contexts_features: 24 \t \n"
     ]
    }
   ],
   "source": [
    "n_users = df.user.nunique()\n",
    "n_items = df.item.nunique()\n",
    "n_contexts = len(context_labels)\n",
    "\n",
    "print(f\"rating with value 1: {df[df.rating == 1]['rating'].count() * 100 / len(df)} %\")\n",
    "print(f\"users: {n_users} \\t items: {n_items} \\t rating: {len(df)} \\t user_features: {len(user_labels)} \\t items_features: {len(item_labels)} \\t contexts_features: {n_contexts} \\t \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "models_eval_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings = coo_matrix((df_mf['rating'].astype(np.float32),\n",
    "                     (df_mf['item'],\n",
    "                      df_mf['user']))).tocsr()\n",
    "\n",
    "auc = 0\n",
    "train, test = train_test_split(ratings, train_percentage=0.80)\n",
    "for split in range(n_splits):\n",
    "    model = AlternatingLeastSquares(factors=256, regularization=8, iterations=10, calculate_training_loss=True)\n",
    "    model.fit(train, show_progress=False)\n",
    "    auc = auc + rs_models.mf_AUC(model, train, test)\n",
    "auc = auc / n_splits\n",
    "print(f\"ALS \\t AUC: {auc}\")\n",
    "models_eval_metrics['ALS'] = [0, auc, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'n_contexts': n_contexts,\n",
    "    'learn_rate': 0.001,\n",
    "    'batch_size': 10000,\n",
    "    'epochs': 5\n",
    "}   \n",
    "\n",
    "\n",
    "std_dev, accuracy, auc, precision, recall = rs_models.kfold_train(rs_models.NeuMF, param, df, n_splits=n_splits)\n",
    "models_eval_metrics['NeuMF'] = [accuracy, auc, precision, recall]\n",
    "print(f\"NeuMF \\t accuracy: {accuracy*100}% \\t AUC: {auc} \\t precision: {precision} \\t recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECAM NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'n_contexts': n_contexts,\n",
    "    'learn_rate': 0.001,\n",
    "    'batch_size': 2048,\n",
    "    'epochs': 5\n",
    "}  \n",
    "\n",
    "std_dev, accuracy, auc, precision, recall = rs_models.kfold_train(rs_models.ECAM_NeuMF, param, df, context_labels=context_labels, n_splits=n_splits)\n",
    "models_eval_metrics['ECAM NeuMF'] = [accuracy, auc, precision, recall]\n",
    "print(f\"ECAM NeuMF \\t accuracy: {accuracy*100}% \\t AUC: {auc} \\t precision: {precision} \\t recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnet = KerasClassifier(build_fn=rs_models.mobile_model, neurons=100, layers=3, learn_rate=0.005, epochs=10, batch_size=64, verbose=False)\n",
    "x = df[item_labels + user_labels + context_labels]\n",
    "y = df['rating']\n",
    "\n",
    "scores = cross_validate(ffnet, x, y, cv=KFold(shuffle=True, n_splits=n_splits, random_state=42), scoring=['accuracy', 'roc_auc', 'precision', 'recall'])\n",
    "\n",
    "accuracy = np.average(scores['test_accuracy'])\n",
    "auc = np.average(scores['test_roc_auc'])\n",
    "precision = np.average(scores['test_precision'])\n",
    "recall = np.average(scores['test_recall'])\n",
    "models_eval_metrics['Classifier'] = [accuracy, auc, precision, recall]\n",
    "\n",
    "print(f\"Classifier \\t accuracy: {accuracy*100}% \\t AUC: {auc} \\t precision: {precision} \\t recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = len(models_eval_metrics) # number of different models\n",
    "models_name = [x[0] for x in models_eval_metrics.items()] \n",
    "accuracy = [x[0] for x in models_eval_metrics.values()]\n",
    "auc = [x[1] for x in models_eval_metrics.values()]\n",
    "precision = [x[2] for x in models_eval_metrics.values()]\n",
    "recall = [x[3] for x in models_eval_metrics.values()]\n",
    "\n",
    "index = np.arange(n_models)\n",
    "bar_width = 0.50\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.bar(index, auc, bar_width, color='tab:orange', label='AUC')\n",
    "\n",
    "for i, value in enumerate(auc): # add metric value at the top of the bar\n",
    "    plt.text(i-bar_width/3, value + 0.01, str(round(value, 4))) # parameters are x position, y position, value\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Prediction results')\n",
    "plt.xticks(index, models_name) # labels position\n",
    "plt.legend(bbox_to_anchor=(1.2, 1))\n",
    "plt.grid(True)\n",
    "plt.savefig('prediction_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier only on some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5820813079169682\n",
      "0.7497580472918305\n",
      "0.8043483972001759\n"
     ]
    }
   ],
   "source": [
    "train_labels = [item_labels, item_labels+user_labels, item_labels+user_labels+context_labels]\n",
    "y = df['rating']\n",
    "results = []\n",
    "\n",
    "for x_labels in train_labels:\n",
    "    x = df[x_labels]\n",
    "    ffnet = KerasClassifier(build_fn=rs_models.mobile_model, neurons=100, layers=3, learn_rate=0.005, epochs=10, batch_size=64, verbose=False)\n",
    "    scores = cross_validate(ffnet, x, y, cv=KFold(shuffle=True, n_splits=n_splits, random_state=42), scoring=['accuracy', 'roc_auc', 'precision', 'recall'])\n",
    "    auc = np.average(scores['test_roc_auc'])\n",
    "    print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
