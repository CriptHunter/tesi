{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "democratic-alliance",
   "metadata": {
    "id": "dn7d0bVYO2H1"
   },
   "source": [
    "# Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-liquid",
   "metadata": {},
   "source": [
    "**User:**\n",
    "- name\n",
    "- friend list\n",
    "**Tip:** Niente\n",
    "\n",
    "**Review:**\n",
    "- user id\n",
    "- business id\n",
    "- stars\n",
    "- date\n",
    "\n",
    "**Checkin:** Niente\n",
    "\n",
    "**Business:**\n",
    "- categories\n",
    "- attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-legislature",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets():\n",
    "    if os.path.exists('Datasets/yelp dataset/yelp_dataset_merged.csv'):\n",
    "        print(\"merged dataset already exists, skipping merge...\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_json('Datasets/yelp dataset/yelp_academic_dataset_business.json', lines=True)\n",
    "    df = df[['business_id', 'city', 'categories', 'attributes']]\n",
    "    df = df[df.city == 'Toronto'] # keep only Toronto, the city with more rating\n",
    "    df = df[df['categories'].str.contains('Restaurant.*')==True].reset_index(drop=True) # keep only restaurant\n",
    "\n",
    "    chunk_list = []  # append each dataframe chunk here \n",
    "    for chunk in tqdm(pd.read_json('Datasets/yelp dataset/yelp_academic_dataset_review.json', lines=True, chunksize=500000)):\n",
    "        chunk = chunk[['user_id', 'business_id', 'stars', 'date']]\n",
    "        chunk = pd.merge(chunk, df, on='business_id') # merge business dataset and review dataset chunk\n",
    "        chunk_list.append(chunk)\n",
    "    df = pd.concat(chunk_list)\n",
    "    df.to_csv('Datasets/yelp dataset/yelp_dataset_merged.csv', index = False) # save dataset to CSV file\n",
    "    \n",
    "merge_datasets()\n",
    "df = pd.read_csv('Datasets/yelp dataset/yelp_dataset_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'user_id':'user', 'business_id':'item', 'stars':'rating'})\n",
    "#df = df[(df.groupby('user')['user'].transform('size') > 10) & (df.groupby('item')['item'].transform('size') > 10)]\n",
    "df = df[(df.groupby('user')['user'].transform('size') > 20)]\n",
    "df['rating'] = df['rating'].apply(lambda x: 1 if x > 3 else 0) # make rating binary\n",
    "df = df.drop(columns='city') # drop city column since we are using only Toronto\n",
    "df = df.dropna() # drop any row with NaN values\n",
    "\n",
    "#df = df.groupby('rating').apply(lambda x: x.sample(1000))\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f'row: {len(df)} \\t user: {df.user.nunique()} \\t item:{df.item.nunique()}')\n",
    "\n",
    "# make user and items id start from 0\n",
    "df.user = pd.factorize(df.user)[0]\n",
    "df.item = pd.factorize(df.item)[0]\n",
    "\n",
    "\n",
    "print(f' rating: {df.rating.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-pleasure",
   "metadata": {},
   "source": [
    "## Context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_from_date(date):\n",
    "    seasons = np.arange(12)\n",
    "    seasons = seasons.reshape(4, 3) # reshape to a 2D matrix\n",
    "    i, j = np.where(seasons == date.month - 1) # get row where month appears\n",
    "    return i[0] + 1 # 1 = winter, 2 = spring, 3 = summer, 4 = autumn\n",
    "\n",
    "def holiday_from_date(date):\n",
    "    us_holidays = holidays.Canada()\n",
    "    return date in us_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']) # convert from string to datetime\n",
    "df['season'] = df['date'].apply(season_from_date)\n",
    "df['weekday'] = df['date'].dt.dayofweek # get day of the week from date (0 to 6)\n",
    "df['weekend'] = (df['weekday'] == 6) | (df['weekday'] == 5) # if is weekend from week day\n",
    "df['holiday'] = df['date'].apply(holiday_from_date) # get holiday in Canada from date\n",
    "df = df.drop(columns=['date']) \n",
    "context = 'season weekday weekend holiday'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-brass",
   "metadata": {},
   "source": [
    "## Item Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"attributes\"] = df[\"attributes\"].apply(lambda x : dict(eval(x))) # convert to dict\n",
    "df = df.join(pd.json_normalize(df.attributes)).drop('attributes', axis=1) # expand dictionaries to new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for c in df.columns:\n",
    "    print('-'*10 + c + '-'*10)\n",
    "    print(df[c].value_counts())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = 'Caters RestaurantsAttire RestaurantsPriceRange2 HasTV NoiseLevel RestaurantsDelivery RestaurantsReservations GoodForKids RestaurantsTakeOut Alcohol OutdoorSeating RestaurantsGoodForGroups GoodForMeal Ambience'.split()\n",
    "df = df[['user', 'item', 'rating', 'categories'] + context + attributes]  # keep only some attributes\n",
    "df = df.replace(to_replace='None', value=np.nan) # replace None strings with NaN\n",
    "df = df.dropna() # Drop NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in 'GoodForMeal Ambience'.split(): # some attributes need to be expanded again\n",
    "    df[col] = df[col].apply(lambda x : dict(eval(x))) # convert to dict\n",
    "    df = df.join(pd.json_normalize(df[col])).drop(col, axis=1) # expand dictionaries to new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace='None', value=False) # replace None with False\n",
    "df = df.fillna(False) # replace NaN with False\n",
    "df = df.dropna() # Drop NaN value\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "removable-pension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user', 'item', 'rating', 'categories', 'season', 'weekday', 'weekend',\n",
       "       'holiday', 'Caters', 'RestaurantsAttire', 'RestaurantsPriceRange2',\n",
       "       'HasTV', 'NoiseLevel', 'RestaurantsDelivery', 'RestaurantsReservations',\n",
       "       'GoodForKids', 'RestaurantsTakeOut', 'Alcohol', 'OutdoorSeating',\n",
       "       'RestaurantsGoodForGroups', 'dessert', 'latenight', 'lunch', 'dinner',\n",
       "       'brunch', 'breakfast', 'romantic', 'intimate', 'classy', 'hipster',\n",
       "       'divey', 'touristy', 'trendy', 'upscale', 'casual'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "aboriginal-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_by_rating(df, column):\n",
    "    fav_values = np.zeros((df.user.nunique(), 2), dtype=object)\n",
    "    for user in df.user.unique():\n",
    "        # group by column unique values and sum ratings\n",
    "        grouped = df[['user', column, 'rating']][df.user == user].groupby(['user', column]).sum().sort_values('rating')\n",
    "        fav_val = grouped.tail(1).index.get_level_values(1).tolist()[0] # get value with highest rating sum\n",
    "        fav_values[user,:] = [user, fav_val] # add to numpy array of (user, fav_val)\n",
    "    return pd.DataFrame(fav_values, columns=['user', 'user_'+column]) # numpy to dataframe\n",
    "\n",
    "df = pd.merge(df, get_user_feature_by_rating(df, 'RestaurantsPriceRange2'), on=['user'])\n",
    "df = pd.merge(df, get_user_feature_by_rating(df, 'Alcohol'), on=['user'])\n",
    "df = pd.merge(df, get_user_feature_by_rating(df, 'RestaurantsDelivery'), on=['user'])\n",
    "df = pd.merge(df, get_user_feature_by_rating(df, 'RestaurantsReservations'), on=['user'])\n",
    "df = pd.merge(df, get_user_feature_by_rating(df, 'GoodForKids'), on=['user'])\n",
    "df = pd.merge(df, get_user_feature_by_rating(df, 'RestaurantsGoodForGroups'), on=['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "phantom-malpractice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_RestaurantsPriceRange2',\n",
       " 'user_Alcohol',\n",
       " 'user_RestaurantsDelivery',\n",
       " 'user_RestaurantsReservations',\n",
       " 'user_GoodForKids',\n",
       " 'user_RestaurantsGoodForGroups']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = df.columns[8:-6].to_list()\n",
    "user_features = df.columns[-6:].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-audience",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "eastern-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical data to one-hot encoding\n",
    "for col in context + attributes + user_features:\n",
    "  df = pd.get_dummies(df, columns=[col], prefix = [col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-airfare",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dynamic-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories = pd.Series(df['categories']).str.get_dummies(',')\n",
    "df = pd.concat([df, df_categories], axis=1)\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['categories']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "thousand-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.user = pd.factorize(df.user)[0]\n",
    "df.item = pd.factorize(df.item)[0]\n",
    "df.to_csv('Datasets/yelp dataset/yelp_final.csv', index = False) \n",
    "df = df.drop_duplicates(subset=['user', 'item']) # drop duplicates for matrix factorization\n",
    "df = df[['user', 'item', 'rating']]\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv('Datasets/yelp dataset/yelp_matrix_factorization.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
